{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6rc1"
    },
    "colab": {
      "name": "HM3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV-4Pe0yq2en"
      },
      "source": [
        "# Home 3: Build a CNN for image recognition.\n",
        "\n",
        "### Name: [Your-Name?]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIX52Gchq2ep"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zcu9Lybq2eq"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpO3F1iOq2eq"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz-qgaOTq2eq",
        "outputId": "36fe75a8-ec27-41d1-96e0-f7bdb83dde18"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoTmvmKVq2er"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b2HphnFq2er",
        "outputId": "7ff44817-77ca-4243-c73c-f89ba478e76d"
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    n = y.shape[0]\n",
        "    new_y = numpy.zeros((n,num_class))\n",
        "    for i in range(n):\n",
        "        p = y[i][0]\n",
        "        new_y[i][p] = 1\n",
        "    return new_y\n",
        "        \n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS6c4gzTq2es"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XzTd_gLq2es"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDeeQojwq2es",
        "outputId": "05241f6d-c9f1-4bf9-f3a9-dd3bb22be476"
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2reRFcrMq2et"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HqFd9Nfq2et"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ9Lyjaoq2et",
        "outputId": "a4b65265-baf1-406c-c966-901573067093"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,BatchNormalization,Activation,Dropout\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(300))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 300)               614700    \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                3010      \n",
            "=================================================================\n",
            "Total params: 898,334\n",
            "Trainable params: 896,902\n",
            "Non-trainable params: 1,432\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP2ctjQZq2eu"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 1E-5 # to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK8GpCwQf2G1"
      },
      "source": [
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     rotation_range = 40,\n",
        "#     width_shift_range = 0.2,\n",
        "#     height_shift_range = 0.2,\n",
        "#     shear_range = 0.2,\n",
        "#     zoom_range = 0.2,\n",
        "#     horizontal_flip=True,\n",
        "#     vertical_flip=True,)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abg7RTtAiilg"
      },
      "source": [
        "# model.fit(train_datagen.flow(x_tr, y_tr, batch_size=32), epochs=300, validation_data=(x_val, y_val))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0oXUhD6q2eu",
        "outputId": "5ac3fb42-3a1c-470a-df76-03c19624774b"
      },
      "source": [
        "history = model.fit(x_tr, y_tr, batch_size=32, epochs=250, validation_data=(x_val, y_val))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1250/1250 [==============================] - 9s 6ms/step - loss: 2.3675 - acc: 0.1782 - val_loss: 1.7185 - val_acc: 0.3846\n",
            "Epoch 2/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.8392 - acc: 0.3363 - val_loss: 1.5559 - val_acc: 0.4393\n",
            "Epoch 3/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6847 - acc: 0.3917 - val_loss: 1.4556 - val_acc: 0.4769\n",
            "Epoch 4/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5767 - acc: 0.4319 - val_loss: 1.3837 - val_acc: 0.5076\n",
            "Epoch 5/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4968 - acc: 0.4605 - val_loss: 1.3301 - val_acc: 0.5264\n",
            "Epoch 6/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4511 - acc: 0.4804 - val_loss: 1.2934 - val_acc: 0.5431\n",
            "Epoch 7/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3861 - acc: 0.4932 - val_loss: 1.2505 - val_acc: 0.5588\n",
            "Epoch 8/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3505 - acc: 0.5135 - val_loss: 1.2248 - val_acc: 0.5668\n",
            "Epoch 9/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.2987 - acc: 0.5359 - val_loss: 1.1921 - val_acc: 0.5811\n",
            "Epoch 10/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2730 - acc: 0.5428 - val_loss: 1.1653 - val_acc: 0.5890\n",
            "Epoch 11/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2322 - acc: 0.5594 - val_loss: 1.1548 - val_acc: 0.5917\n",
            "Epoch 12/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2179 - acc: 0.5628 - val_loss: 1.1202 - val_acc: 0.6060\n",
            "Epoch 13/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1888 - acc: 0.5726 - val_loss: 1.1023 - val_acc: 0.6168\n",
            "Epoch 14/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1622 - acc: 0.5852 - val_loss: 1.0761 - val_acc: 0.6249\n",
            "Epoch 15/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1272 - acc: 0.5998 - val_loss: 1.0586 - val_acc: 0.6289\n",
            "Epoch 16/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1176 - acc: 0.6025 - val_loss: 1.0566 - val_acc: 0.6296\n",
            "Epoch 17/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0901 - acc: 0.6113 - val_loss: 1.0304 - val_acc: 0.6380\n",
            "Epoch 18/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0640 - acc: 0.6213 - val_loss: 1.0209 - val_acc: 0.6398\n",
            "Epoch 19/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0582 - acc: 0.6219 - val_loss: 0.9985 - val_acc: 0.6480\n",
            "Epoch 20/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0451 - acc: 0.6314 - val_loss: 0.9963 - val_acc: 0.6482\n",
            "Epoch 21/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0263 - acc: 0.6365 - val_loss: 0.9781 - val_acc: 0.6567\n",
            "Epoch 22/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.0168 - acc: 0.6374 - val_loss: 0.9666 - val_acc: 0.6576\n",
            "Epoch 23/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9850 - acc: 0.6513 - val_loss: 0.9672 - val_acc: 0.6583\n",
            "Epoch 24/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.9791 - acc: 0.6586 - val_loss: 0.9523 - val_acc: 0.6650\n",
            "Epoch 25/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9680 - acc: 0.6587 - val_loss: 0.9403 - val_acc: 0.6657\n",
            "Epoch 26/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9498 - acc: 0.6639 - val_loss: 0.9272 - val_acc: 0.6692\n",
            "Epoch 27/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.9488 - acc: 0.6685 - val_loss: 0.9163 - val_acc: 0.6735\n",
            "Epoch 28/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9347 - acc: 0.6732 - val_loss: 0.9136 - val_acc: 0.6773\n",
            "Epoch 29/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9235 - acc: 0.6738 - val_loss: 0.9090 - val_acc: 0.6773\n",
            "Epoch 30/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.9089 - acc: 0.6776 - val_loss: 0.8988 - val_acc: 0.6805\n",
            "Epoch 31/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.9077 - acc: 0.6779 - val_loss: 0.8894 - val_acc: 0.6789\n",
            "Epoch 32/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8866 - acc: 0.6870 - val_loss: 0.8866 - val_acc: 0.6855\n",
            "Epoch 33/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.8801 - acc: 0.6902 - val_loss: 0.8807 - val_acc: 0.6867\n",
            "Epoch 34/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.8800 - acc: 0.6896 - val_loss: 0.8775 - val_acc: 0.6894\n",
            "Epoch 35/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8646 - acc: 0.6956 - val_loss: 0.8670 - val_acc: 0.6905\n",
            "Epoch 36/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8432 - acc: 0.7057 - val_loss: 0.8683 - val_acc: 0.6890\n",
            "Epoch 37/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8430 - acc: 0.7016 - val_loss: 0.8586 - val_acc: 0.6936\n",
            "Epoch 38/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8391 - acc: 0.7054 - val_loss: 0.8552 - val_acc: 0.6954\n",
            "Epoch 39/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.8240 - acc: 0.7072 - val_loss: 0.8674 - val_acc: 0.6891\n",
            "Epoch 40/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.8186 - acc: 0.7110 - val_loss: 0.8429 - val_acc: 0.6967\n",
            "Epoch 41/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8056 - acc: 0.7155 - val_loss: 0.8457 - val_acc: 0.6950\n",
            "Epoch 42/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7932 - acc: 0.7249 - val_loss: 0.8321 - val_acc: 0.7010\n",
            "Epoch 43/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7933 - acc: 0.7205 - val_loss: 0.8286 - val_acc: 0.7043\n",
            "Epoch 44/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7896 - acc: 0.7200 - val_loss: 0.8296 - val_acc: 0.7033\n",
            "Epoch 45/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7761 - acc: 0.7268 - val_loss: 0.8231 - val_acc: 0.7047\n",
            "Epoch 46/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7745 - acc: 0.7266 - val_loss: 0.8202 - val_acc: 0.7071\n",
            "Epoch 47/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7571 - acc: 0.7333 - val_loss: 0.8110 - val_acc: 0.7075\n",
            "Epoch 48/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7634 - acc: 0.7322 - val_loss: 0.8162 - val_acc: 0.7078\n",
            "Epoch 49/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7457 - acc: 0.7398 - val_loss: 0.8085 - val_acc: 0.7125\n",
            "Epoch 50/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7507 - acc: 0.7401 - val_loss: 0.8148 - val_acc: 0.7125\n",
            "Epoch 51/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7325 - acc: 0.7392 - val_loss: 0.8068 - val_acc: 0.7116\n",
            "Epoch 52/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7290 - acc: 0.7436 - val_loss: 0.7958 - val_acc: 0.7142\n",
            "Epoch 53/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7212 - acc: 0.7467 - val_loss: 0.7932 - val_acc: 0.7171\n",
            "Epoch 54/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7239 - acc: 0.7471 - val_loss: 0.7937 - val_acc: 0.7184\n",
            "Epoch 55/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7175 - acc: 0.7478 - val_loss: 0.7912 - val_acc: 0.7176\n",
            "Epoch 56/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7033 - acc: 0.7525 - val_loss: 0.7947 - val_acc: 0.7205\n",
            "Epoch 57/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6986 - acc: 0.7536 - val_loss: 0.7840 - val_acc: 0.7203\n",
            "Epoch 58/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6986 - acc: 0.7537 - val_loss: 0.8019 - val_acc: 0.7186\n",
            "Epoch 59/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6850 - acc: 0.7568 - val_loss: 0.7928 - val_acc: 0.7199\n",
            "Epoch 60/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6667 - acc: 0.7662 - val_loss: 0.7871 - val_acc: 0.7216\n",
            "Epoch 61/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6855 - acc: 0.7581 - val_loss: 0.7786 - val_acc: 0.7244\n",
            "Epoch 62/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6676 - acc: 0.7612 - val_loss: 0.7805 - val_acc: 0.7257\n",
            "Epoch 63/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6688 - acc: 0.7697 - val_loss: 0.7779 - val_acc: 0.7243\n",
            "Epoch 64/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6624 - acc: 0.7672 - val_loss: 0.7748 - val_acc: 0.7262\n",
            "Epoch 65/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6614 - acc: 0.7673 - val_loss: 0.7683 - val_acc: 0.7292\n",
            "Epoch 66/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6497 - acc: 0.7708 - val_loss: 0.7647 - val_acc: 0.7310\n",
            "Epoch 67/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6423 - acc: 0.7728 - val_loss: 0.7686 - val_acc: 0.7290\n",
            "Epoch 68/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6352 - acc: 0.7774 - val_loss: 0.7682 - val_acc: 0.7321\n",
            "Epoch 69/250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6365 - acc: 0.7790 - val_loss: 0.7610 - val_acc: 0.7334\n",
            "Epoch 70/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6317 - acc: 0.7780 - val_loss: 0.7750 - val_acc: 0.7278\n",
            "Epoch 71/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6286 - acc: 0.7758 - val_loss: 0.7603 - val_acc: 0.7322\n",
            "Epoch 72/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6071 - acc: 0.7868 - val_loss: 0.7681 - val_acc: 0.7321\n",
            "Epoch 73/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6179 - acc: 0.7801 - val_loss: 0.7577 - val_acc: 0.7319\n",
            "Epoch 74/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6234 - acc: 0.7837 - val_loss: 0.7572 - val_acc: 0.7367\n",
            "Epoch 75/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6031 - acc: 0.7883 - val_loss: 0.7551 - val_acc: 0.7358\n",
            "Epoch 76/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6067 - acc: 0.7866 - val_loss: 0.7569 - val_acc: 0.7364\n",
            "Epoch 77/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5915 - acc: 0.7912 - val_loss: 0.7546 - val_acc: 0.7385\n",
            "Epoch 78/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5916 - acc: 0.7891 - val_loss: 0.7537 - val_acc: 0.7387\n",
            "Epoch 79/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5826 - acc: 0.7934 - val_loss: 0.7512 - val_acc: 0.7381\n",
            "Epoch 80/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5868 - acc: 0.7911 - val_loss: 0.7583 - val_acc: 0.7397\n",
            "Epoch 81/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5760 - acc: 0.7939 - val_loss: 0.7596 - val_acc: 0.7405\n",
            "Epoch 82/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5749 - acc: 0.7992 - val_loss: 0.7487 - val_acc: 0.7407\n",
            "Epoch 83/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5717 - acc: 0.7972 - val_loss: 0.7491 - val_acc: 0.7426\n",
            "Epoch 84/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5609 - acc: 0.8031 - val_loss: 0.7491 - val_acc: 0.7429\n",
            "Epoch 85/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5592 - acc: 0.8012 - val_loss: 0.7445 - val_acc: 0.7412\n",
            "Epoch 86/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5588 - acc: 0.8019 - val_loss: 0.7461 - val_acc: 0.7429\n",
            "Epoch 87/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5552 - acc: 0.8047 - val_loss: 0.7574 - val_acc: 0.7396\n",
            "Epoch 88/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5470 - acc: 0.8059 - val_loss: 0.7552 - val_acc: 0.7441\n",
            "Epoch 89/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5383 - acc: 0.8095 - val_loss: 0.7425 - val_acc: 0.7450\n",
            "Epoch 90/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5375 - acc: 0.8093 - val_loss: 0.7517 - val_acc: 0.7434\n",
            "Epoch 91/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5427 - acc: 0.8088 - val_loss: 0.7453 - val_acc: 0.7462\n",
            "Epoch 92/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5253 - acc: 0.8143 - val_loss: 0.7374 - val_acc: 0.7454\n",
            "Epoch 93/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5283 - acc: 0.8106 - val_loss: 0.7428 - val_acc: 0.7446\n",
            "Epoch 94/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5265 - acc: 0.8132 - val_loss: 0.7464 - val_acc: 0.7429\n",
            "Epoch 95/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5195 - acc: 0.8160 - val_loss: 0.7495 - val_acc: 0.7449\n",
            "Epoch 96/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5186 - acc: 0.8178 - val_loss: 0.7567 - val_acc: 0.7457\n",
            "Epoch 97/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5141 - acc: 0.8171 - val_loss: 0.7442 - val_acc: 0.7450\n",
            "Epoch 98/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5124 - acc: 0.8206 - val_loss: 0.7518 - val_acc: 0.7472\n",
            "Epoch 99/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5054 - acc: 0.8206 - val_loss: 0.7418 - val_acc: 0.7476\n",
            "Epoch 100/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4974 - acc: 0.8254 - val_loss: 0.7469 - val_acc: 0.7472\n",
            "Epoch 101/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4994 - acc: 0.8234 - val_loss: 0.7447 - val_acc: 0.7500\n",
            "Epoch 102/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4908 - acc: 0.8248 - val_loss: 0.7363 - val_acc: 0.7501\n",
            "Epoch 103/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4901 - acc: 0.8285 - val_loss: 0.7451 - val_acc: 0.7503\n",
            "Epoch 104/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4814 - acc: 0.8328 - val_loss: 0.7516 - val_acc: 0.7488\n",
            "Epoch 105/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4797 - acc: 0.8316 - val_loss: 0.7460 - val_acc: 0.7494\n",
            "Epoch 106/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4786 - acc: 0.8354 - val_loss: 0.7608 - val_acc: 0.7468\n",
            "Epoch 107/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4792 - acc: 0.8302 - val_loss: 0.7570 - val_acc: 0.7475\n",
            "Epoch 108/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4744 - acc: 0.8317 - val_loss: 0.7349 - val_acc: 0.7526\n",
            "Epoch 109/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4671 - acc: 0.8354 - val_loss: 0.7394 - val_acc: 0.7515\n",
            "Epoch 110/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4585 - acc: 0.8367 - val_loss: 0.7367 - val_acc: 0.7514\n",
            "Epoch 111/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4558 - acc: 0.8378 - val_loss: 0.7388 - val_acc: 0.7520\n",
            "Epoch 112/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4575 - acc: 0.8430 - val_loss: 0.7412 - val_acc: 0.7508\n",
            "Epoch 113/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4538 - acc: 0.8393 - val_loss: 0.7393 - val_acc: 0.7517\n",
            "Epoch 114/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4531 - acc: 0.8411 - val_loss: 0.7465 - val_acc: 0.7521\n",
            "Epoch 115/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4505 - acc: 0.8395 - val_loss: 0.7300 - val_acc: 0.7561\n",
            "Epoch 116/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4392 - acc: 0.8443 - val_loss: 0.7389 - val_acc: 0.7519\n",
            "Epoch 117/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4398 - acc: 0.8473 - val_loss: 0.7381 - val_acc: 0.7554\n",
            "Epoch 118/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4361 - acc: 0.8465 - val_loss: 0.7572 - val_acc: 0.7520\n",
            "Epoch 119/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4278 - acc: 0.8483 - val_loss: 0.7332 - val_acc: 0.7549\n",
            "Epoch 120/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4244 - acc: 0.8495 - val_loss: 0.7414 - val_acc: 0.7535\n",
            "Epoch 121/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4266 - acc: 0.8469 - val_loss: 0.7375 - val_acc: 0.7567\n",
            "Epoch 122/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4275 - acc: 0.8468 - val_loss: 0.7447 - val_acc: 0.7549\n",
            "Epoch 123/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4319 - acc: 0.8471 - val_loss: 0.7405 - val_acc: 0.7561\n",
            "Epoch 124/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4191 - acc: 0.8517 - val_loss: 0.7400 - val_acc: 0.7538\n",
            "Epoch 125/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4100 - acc: 0.8541 - val_loss: 0.7355 - val_acc: 0.7567\n",
            "Epoch 126/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4129 - acc: 0.8547 - val_loss: 0.7545 - val_acc: 0.7538\n",
            "Epoch 127/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4129 - acc: 0.8531 - val_loss: 0.7435 - val_acc: 0.7585\n",
            "Epoch 128/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4061 - acc: 0.8560 - val_loss: 0.7529 - val_acc: 0.7560\n",
            "Epoch 129/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4141 - acc: 0.8538 - val_loss: 0.7416 - val_acc: 0.7559\n",
            "Epoch 130/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.4003 - acc: 0.8583 - val_loss: 0.7511 - val_acc: 0.7549\n",
            "Epoch 131/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3970 - acc: 0.8607 - val_loss: 0.7438 - val_acc: 0.7525\n",
            "Epoch 132/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3999 - acc: 0.8600 - val_loss: 0.7526 - val_acc: 0.7566\n",
            "Epoch 133/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3935 - acc: 0.8604 - val_loss: 0.7438 - val_acc: 0.7561\n",
            "Epoch 134/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3945 - acc: 0.8640 - val_loss: 0.7424 - val_acc: 0.7578\n",
            "Epoch 135/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3855 - acc: 0.8677 - val_loss: 0.7480 - val_acc: 0.7576\n",
            "Epoch 136/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3918 - acc: 0.8609 - val_loss: 0.7423 - val_acc: 0.7571\n",
            "Epoch 137/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3800 - acc: 0.8656 - val_loss: 0.7450 - val_acc: 0.7569\n",
            "Epoch 138/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3874 - acc: 0.8635 - val_loss: 0.7448 - val_acc: 0.7536\n",
            "Epoch 139/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3723 - acc: 0.8695 - val_loss: 0.7420 - val_acc: 0.7545\n",
            "Epoch 140/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3714 - acc: 0.8715 - val_loss: 0.7491 - val_acc: 0.7560\n",
            "Epoch 141/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3665 - acc: 0.8711 - val_loss: 0.7515 - val_acc: 0.7555\n",
            "Epoch 142/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3607 - acc: 0.8728 - val_loss: 0.7614 - val_acc: 0.7543\n",
            "Epoch 143/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3702 - acc: 0.8685 - val_loss: 0.7824 - val_acc: 0.7528\n",
            "Epoch 144/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3681 - acc: 0.8714 - val_loss: 0.7458 - val_acc: 0.7580\n",
            "Epoch 145/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3669 - acc: 0.8697 - val_loss: 0.7537 - val_acc: 0.7595\n",
            "Epoch 146/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3642 - acc: 0.8700 - val_loss: 0.7670 - val_acc: 0.7540\n",
            "Epoch 147/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3605 - acc: 0.8725 - val_loss: 0.7562 - val_acc: 0.7579\n",
            "Epoch 148/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3564 - acc: 0.8747 - val_loss: 0.7534 - val_acc: 0.7586\n",
            "Epoch 149/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3534 - acc: 0.8753 - val_loss: 0.7546 - val_acc: 0.7586\n",
            "Epoch 150/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3499 - acc: 0.8748 - val_loss: 0.7511 - val_acc: 0.7580\n",
            "Epoch 151/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3477 - acc: 0.8758 - val_loss: 0.7511 - val_acc: 0.7595\n",
            "Epoch 152/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3517 - acc: 0.8745 - val_loss: 0.7584 - val_acc: 0.7593\n",
            "Epoch 153/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3370 - acc: 0.8828 - val_loss: 0.7565 - val_acc: 0.7568\n",
            "Epoch 154/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3430 - acc: 0.8779 - val_loss: 0.7609 - val_acc: 0.7588\n",
            "Epoch 155/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3343 - acc: 0.8816 - val_loss: 0.7643 - val_acc: 0.7563\n",
            "Epoch 156/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3328 - acc: 0.8820 - val_loss: 0.7677 - val_acc: 0.7580\n",
            "Epoch 157/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3325 - acc: 0.8815 - val_loss: 0.7816 - val_acc: 0.7558\n",
            "Epoch 158/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3283 - acc: 0.8865 - val_loss: 0.7728 - val_acc: 0.7585\n",
            "Epoch 159/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3178 - acc: 0.8870 - val_loss: 0.7751 - val_acc: 0.7565\n",
            "Epoch 160/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3269 - acc: 0.8846 - val_loss: 0.7655 - val_acc: 0.7595\n",
            "Epoch 161/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3193 - acc: 0.8889 - val_loss: 0.7607 - val_acc: 0.7589\n",
            "Epoch 162/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3245 - acc: 0.8844 - val_loss: 0.7677 - val_acc: 0.7557\n",
            "Epoch 163/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3161 - acc: 0.8856 - val_loss: 0.7665 - val_acc: 0.7604\n",
            "Epoch 164/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3129 - acc: 0.8898 - val_loss: 0.7719 - val_acc: 0.7585\n",
            "Epoch 165/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3150 - acc: 0.8887 - val_loss: 0.7879 - val_acc: 0.7567\n",
            "Epoch 166/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3218 - acc: 0.8843 - val_loss: 0.7647 - val_acc: 0.7606\n",
            "Epoch 167/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3152 - acc: 0.8892 - val_loss: 0.7730 - val_acc: 0.7565\n",
            "Epoch 168/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3067 - acc: 0.8896 - val_loss: 0.7792 - val_acc: 0.7591\n",
            "Epoch 169/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3032 - acc: 0.8937 - val_loss: 0.7750 - val_acc: 0.7588\n",
            "Epoch 170/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2998 - acc: 0.8942 - val_loss: 0.7719 - val_acc: 0.7614\n",
            "Epoch 171/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2980 - acc: 0.8945 - val_loss: 0.7700 - val_acc: 0.7590\n",
            "Epoch 172/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2892 - acc: 0.8980 - val_loss: 0.7777 - val_acc: 0.7614\n",
            "Epoch 173/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.3020 - acc: 0.8939 - val_loss: 0.7869 - val_acc: 0.7613\n",
            "Epoch 174/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2928 - acc: 0.8945 - val_loss: 0.8005 - val_acc: 0.7571\n",
            "Epoch 175/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2860 - acc: 0.8994 - val_loss: 0.7799 - val_acc: 0.7583\n",
            "Epoch 176/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2953 - acc: 0.8961 - val_loss: 0.7803 - val_acc: 0.7586\n",
            "Epoch 177/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2861 - acc: 0.8979 - val_loss: 0.7842 - val_acc: 0.7596\n",
            "Epoch 178/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2923 - acc: 0.8953 - val_loss: 0.7818 - val_acc: 0.7599\n",
            "Epoch 179/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2797 - acc: 0.9014 - val_loss: 0.7828 - val_acc: 0.7623\n",
            "Epoch 180/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2890 - acc: 0.8989 - val_loss: 0.8014 - val_acc: 0.7580\n",
            "Epoch 181/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2823 - acc: 0.9027 - val_loss: 0.8002 - val_acc: 0.7552\n",
            "Epoch 182/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2774 - acc: 0.9022 - val_loss: 0.8154 - val_acc: 0.7526\n",
            "Epoch 183/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2797 - acc: 0.9020 - val_loss: 0.7983 - val_acc: 0.7548\n",
            "Epoch 184/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2745 - acc: 0.9012 - val_loss: 0.7880 - val_acc: 0.7611\n",
            "Epoch 185/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2787 - acc: 0.9015 - val_loss: 0.7807 - val_acc: 0.7597\n",
            "Epoch 186/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2638 - acc: 0.9072 - val_loss: 0.7945 - val_acc: 0.7598\n",
            "Epoch 187/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2681 - acc: 0.9045 - val_loss: 0.7952 - val_acc: 0.7582\n",
            "Epoch 188/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2736 - acc: 0.9028 - val_loss: 0.7866 - val_acc: 0.7607\n",
            "Epoch 189/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2653 - acc: 0.9048 - val_loss: 0.7905 - val_acc: 0.7621\n",
            "Epoch 190/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2651 - acc: 0.9066 - val_loss: 0.7838 - val_acc: 0.7617\n",
            "Epoch 191/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2680 - acc: 0.9050 - val_loss: 0.7942 - val_acc: 0.7607\n",
            "Epoch 192/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2578 - acc: 0.9100 - val_loss: 0.7884 - val_acc: 0.7638\n",
            "Epoch 193/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2619 - acc: 0.9082 - val_loss: 0.7887 - val_acc: 0.7607\n",
            "Epoch 194/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2551 - acc: 0.9094 - val_loss: 0.8012 - val_acc: 0.7617\n",
            "Epoch 195/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2615 - acc: 0.9069 - val_loss: 0.8069 - val_acc: 0.7598\n",
            "Epoch 196/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2636 - acc: 0.9074 - val_loss: 0.8060 - val_acc: 0.7614\n",
            "Epoch 197/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2466 - acc: 0.9135 - val_loss: 0.7969 - val_acc: 0.7589\n",
            "Epoch 198/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2504 - acc: 0.9119 - val_loss: 0.8039 - val_acc: 0.7610\n",
            "Epoch 199/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2434 - acc: 0.9154 - val_loss: 0.8209 - val_acc: 0.7614\n",
            "Epoch 200/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2465 - acc: 0.9127 - val_loss: 0.8168 - val_acc: 0.7616\n",
            "Epoch 201/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2556 - acc: 0.9095 - val_loss: 0.7966 - val_acc: 0.7592\n",
            "Epoch 202/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2514 - acc: 0.9126 - val_loss: 0.8114 - val_acc: 0.7573\n",
            "Epoch 203/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2316 - acc: 0.9191 - val_loss: 0.8291 - val_acc: 0.7606\n",
            "Epoch 204/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2383 - acc: 0.9165 - val_loss: 0.8402 - val_acc: 0.7607\n",
            "Epoch 205/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2398 - acc: 0.9159 - val_loss: 0.7997 - val_acc: 0.7631\n",
            "Epoch 206/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2343 - acc: 0.9177 - val_loss: 0.8083 - val_acc: 0.7613\n",
            "Epoch 207/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2371 - acc: 0.9163 - val_loss: 0.8312 - val_acc: 0.7613\n",
            "Epoch 208/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2422 - acc: 0.9128 - val_loss: 0.8026 - val_acc: 0.7609\n",
            "Epoch 209/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2369 - acc: 0.9182 - val_loss: 0.8224 - val_acc: 0.7601\n",
            "Epoch 210/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2377 - acc: 0.9153 - val_loss: 0.8112 - val_acc: 0.7623\n",
            "Epoch 211/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2313 - acc: 0.9181 - val_loss: 0.8081 - val_acc: 0.7658\n",
            "Epoch 212/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2196 - acc: 0.9237 - val_loss: 0.8265 - val_acc: 0.7556\n",
            "Epoch 213/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2291 - acc: 0.9194 - val_loss: 0.8543 - val_acc: 0.7559\n",
            "Epoch 214/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2303 - acc: 0.9179 - val_loss: 0.8350 - val_acc: 0.7602\n",
            "Epoch 215/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2257 - acc: 0.9209 - val_loss: 0.8194 - val_acc: 0.7633\n",
            "Epoch 216/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2297 - acc: 0.9198 - val_loss: 0.8260 - val_acc: 0.7631\n",
            "Epoch 217/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2256 - acc: 0.9210 - val_loss: 0.8145 - val_acc: 0.7651\n",
            "Epoch 218/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2281 - acc: 0.9200 - val_loss: 0.8076 - val_acc: 0.7665\n",
            "Epoch 219/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2248 - acc: 0.9225 - val_loss: 0.8381 - val_acc: 0.7590\n",
            "Epoch 220/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2171 - acc: 0.9243 - val_loss: 0.8161 - val_acc: 0.7621\n",
            "Epoch 221/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2100 - acc: 0.9278 - val_loss: 0.8300 - val_acc: 0.7652\n",
            "Epoch 222/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2145 - acc: 0.9250 - val_loss: 0.8353 - val_acc: 0.7647\n",
            "Epoch 223/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2099 - acc: 0.9268 - val_loss: 0.8328 - val_acc: 0.7635\n",
            "Epoch 224/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2090 - acc: 0.9276 - val_loss: 0.8630 - val_acc: 0.7554\n",
            "Epoch 225/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2090 - acc: 0.9256 - val_loss: 0.8581 - val_acc: 0.7594\n",
            "Epoch 226/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2056 - acc: 0.9272 - val_loss: 0.8522 - val_acc: 0.7624\n",
            "Epoch 227/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2079 - acc: 0.9267 - val_loss: 0.8234 - val_acc: 0.7613\n",
            "Epoch 228/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2054 - acc: 0.9284 - val_loss: 0.8486 - val_acc: 0.7645\n",
            "Epoch 229/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2102 - acc: 0.9262 - val_loss: 0.8244 - val_acc: 0.7628\n",
            "Epoch 230/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2073 - acc: 0.9248 - val_loss: 0.8480 - val_acc: 0.7616\n",
            "Epoch 231/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2021 - acc: 0.9288 - val_loss: 0.8429 - val_acc: 0.7645\n",
            "Epoch 232/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1989 - acc: 0.9308 - val_loss: 0.8643 - val_acc: 0.7564\n",
            "Epoch 233/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1957 - acc: 0.9317 - val_loss: 0.8517 - val_acc: 0.7624\n",
            "Epoch 234/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.2063 - acc: 0.9256 - val_loss: 0.8653 - val_acc: 0.7572\n",
            "Epoch 235/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1955 - acc: 0.9307 - val_loss: 0.8479 - val_acc: 0.7621\n",
            "Epoch 236/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1925 - acc: 0.9322 - val_loss: 0.8778 - val_acc: 0.7580\n",
            "Epoch 237/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1947 - acc: 0.9331 - val_loss: 0.8562 - val_acc: 0.7625\n",
            "Epoch 238/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1908 - acc: 0.9341 - val_loss: 0.8642 - val_acc: 0.7616\n",
            "Epoch 239/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1941 - acc: 0.9312 - val_loss: 0.8742 - val_acc: 0.7588\n",
            "Epoch 240/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1917 - acc: 0.9326 - val_loss: 0.8536 - val_acc: 0.7594\n",
            "Epoch 241/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1952 - acc: 0.9317 - val_loss: 0.8447 - val_acc: 0.7639\n",
            "Epoch 242/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1844 - acc: 0.9348 - val_loss: 0.8729 - val_acc: 0.7584\n",
            "Epoch 243/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1915 - acc: 0.9323 - val_loss: 0.8459 - val_acc: 0.7643\n",
            "Epoch 244/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1894 - acc: 0.9329 - val_loss: 0.8602 - val_acc: 0.7622\n",
            "Epoch 245/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1886 - acc: 0.9340 - val_loss: 0.8593 - val_acc: 0.7640\n",
            "Epoch 246/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1913 - acc: 0.9328 - val_loss: 0.8573 - val_acc: 0.7639\n",
            "Epoch 247/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1817 - acc: 0.9361 - val_loss: 0.8669 - val_acc: 0.7664\n",
            "Epoch 248/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1896 - acc: 0.9329 - val_loss: 0.8668 - val_acc: 0.7581\n",
            "Epoch 249/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1861 - acc: 0.9357 - val_loss: 0.8749 - val_acc: 0.7626\n",
            "Epoch 250/250\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.1806 - acc: 0.9363 - val_loss: 0.8621 - val_acc: 0.7650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "0OaDtTYUq2eu",
        "outputId": "16641592-b0f5-4f00-a1ad-cf290e818dae"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcX8tIhsq2ev"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3255lJP1q2ev"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8thM4Ou3q2ev"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(300))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 1E-5 # to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYaV_smnq2ev",
        "outputId": "e6b7bd1b-82a3-4911-9a92-2340d6118692"
      },
      "source": [
        "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=200)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1563/1563 [==============================] - 11s 6ms/step - loss: 2.2809 - acc: 0.1971\n",
            "Epoch 2/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7941 - acc: 0.3520\n",
            "Epoch 3/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.6298 - acc: 0.4132\n",
            "Epoch 4/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.5197 - acc: 0.4513\n",
            "Epoch 5/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.4361 - acc: 0.4832\n",
            "Epoch 6/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3828 - acc: 0.5021\n",
            "Epoch 7/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3469 - acc: 0.5175\n",
            "Epoch 8/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.3068 - acc: 0.5340\n",
            "Epoch 9/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2627 - acc: 0.5489\n",
            "Epoch 10/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2346 - acc: 0.5569\n",
            "Epoch 11/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.2127 - acc: 0.5708\n",
            "Epoch 12/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1727 - acc: 0.5863\n",
            "Epoch 13/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1536 - acc: 0.5928\n",
            "Epoch 14/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.1201 - acc: 0.6019\n",
            "Epoch 15/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0913 - acc: 0.6159\n",
            "Epoch 16/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0833 - acc: 0.6139\n",
            "Epoch 17/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0540 - acc: 0.6259\n",
            "Epoch 18/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0446 - acc: 0.6298\n",
            "Epoch 19/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0265 - acc: 0.6380\n",
            "Epoch 20/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0025 - acc: 0.6500\n",
            "Epoch 21/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9917 - acc: 0.6484\n",
            "Epoch 22/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9852 - acc: 0.6534\n",
            "Epoch 23/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9630 - acc: 0.6618\n",
            "Epoch 24/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9574 - acc: 0.6627\n",
            "Epoch 25/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9418 - acc: 0.6691\n",
            "Epoch 26/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9265 - acc: 0.6765\n",
            "Epoch 27/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9161 - acc: 0.6808\n",
            "Epoch 28/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9059 - acc: 0.6822\n",
            "Epoch 29/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8876 - acc: 0.6865\n",
            "Epoch 30/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8743 - acc: 0.6906\n",
            "Epoch 31/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8676 - acc: 0.6942\n",
            "Epoch 32/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8688 - acc: 0.6969\n",
            "Epoch 33/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8534 - acc: 0.6991\n",
            "Epoch 34/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8480 - acc: 0.6992\n",
            "Epoch 35/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8410 - acc: 0.7050\n",
            "Epoch 36/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8254 - acc: 0.7091\n",
            "Epoch 37/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8232 - acc: 0.7119\n",
            "Epoch 38/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8147 - acc: 0.7133\n",
            "Epoch 39/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8016 - acc: 0.7184\n",
            "Epoch 40/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7954 - acc: 0.7220\n",
            "Epoch 41/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7837 - acc: 0.7221\n",
            "Epoch 42/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7809 - acc: 0.7265\n",
            "Epoch 43/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7774 - acc: 0.7300\n",
            "Epoch 44/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7654 - acc: 0.7311\n",
            "Epoch 45/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7529 - acc: 0.7376\n",
            "Epoch 46/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7474 - acc: 0.7360\n",
            "Epoch 47/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7510 - acc: 0.7362\n",
            "Epoch 48/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7380 - acc: 0.7368\n",
            "Epoch 49/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7331 - acc: 0.7446\n",
            "Epoch 50/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7254 - acc: 0.7440\n",
            "Epoch 51/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7080 - acc: 0.7508\n",
            "Epoch 52/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7197 - acc: 0.7467\n",
            "Epoch 53/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6981 - acc: 0.7524\n",
            "Epoch 54/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6940 - acc: 0.7545\n",
            "Epoch 55/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6860 - acc: 0.7599\n",
            "Epoch 56/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6902 - acc: 0.7550\n",
            "Epoch 57/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6871 - acc: 0.7564\n",
            "Epoch 58/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6829 - acc: 0.7605\n",
            "Epoch 59/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6727 - acc: 0.7630\n",
            "Epoch 60/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6558 - acc: 0.7693\n",
            "Epoch 61/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6533 - acc: 0.7712\n",
            "Epoch 62/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6526 - acc: 0.7709\n",
            "Epoch 63/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6526 - acc: 0.7709\n",
            "Epoch 64/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6377 - acc: 0.7753\n",
            "Epoch 65/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6324 - acc: 0.7789\n",
            "Epoch 66/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6375 - acc: 0.7767\n",
            "Epoch 67/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6189 - acc: 0.7832\n",
            "Epoch 68/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6193 - acc: 0.7822\n",
            "Epoch 69/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6127 - acc: 0.7837\n",
            "Epoch 70/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6170 - acc: 0.7828\n",
            "Epoch 71/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6163 - acc: 0.7845\n",
            "Epoch 72/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5933 - acc: 0.7907\n",
            "Epoch 73/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5911 - acc: 0.7925\n",
            "Epoch 74/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5983 - acc: 0.7911\n",
            "Epoch 75/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5907 - acc: 0.7918\n",
            "Epoch 76/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5924 - acc: 0.7937\n",
            "Epoch 77/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5791 - acc: 0.7973\n",
            "Epoch 78/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5799 - acc: 0.7979\n",
            "Epoch 79/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5710 - acc: 0.7986\n",
            "Epoch 80/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5684 - acc: 0.8017\n",
            "Epoch 81/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5576 - acc: 0.8024\n",
            "Epoch 82/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5599 - acc: 0.8038\n",
            "Epoch 83/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5527 - acc: 0.8057\n",
            "Epoch 84/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5535 - acc: 0.8083\n",
            "Epoch 85/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5461 - acc: 0.8084\n",
            "Epoch 86/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5363 - acc: 0.8092\n",
            "Epoch 87/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5323 - acc: 0.8121\n",
            "Epoch 88/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5301 - acc: 0.8143\n",
            "Epoch 89/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5319 - acc: 0.8119\n",
            "Epoch 90/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5209 - acc: 0.8165\n",
            "Epoch 91/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5285 - acc: 0.8140\n",
            "Epoch 92/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5153 - acc: 0.8189\n",
            "Epoch 93/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5216 - acc: 0.8167\n",
            "Epoch 94/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5151 - acc: 0.8180\n",
            "Epoch 95/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5098 - acc: 0.8205\n",
            "Epoch 96/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.5024 - acc: 0.8241\n",
            "Epoch 97/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4992 - acc: 0.8239\n",
            "Epoch 98/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4985 - acc: 0.8236\n",
            "Epoch 99/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4905 - acc: 0.8282\n",
            "Epoch 100/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4903 - acc: 0.8297\n",
            "Epoch 101/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4862 - acc: 0.8274\n",
            "Epoch 102/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4798 - acc: 0.8285\n",
            "Epoch 103/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4847 - acc: 0.8276\n",
            "Epoch 104/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4764 - acc: 0.8308\n",
            "Epoch 105/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4775 - acc: 0.8320\n",
            "Epoch 106/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4698 - acc: 0.8375\n",
            "Epoch 107/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4629 - acc: 0.8371\n",
            "Epoch 108/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4603 - acc: 0.8388\n",
            "Epoch 109/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4610 - acc: 0.8388\n",
            "Epoch 110/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4540 - acc: 0.8406\n",
            "Epoch 111/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4495 - acc: 0.8429\n",
            "Epoch 112/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4544 - acc: 0.8390\n",
            "Epoch 113/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4481 - acc: 0.8428\n",
            "Epoch 114/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4514 - acc: 0.8411\n",
            "Epoch 115/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4371 - acc: 0.8476\n",
            "Epoch 116/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4253 - acc: 0.8480\n",
            "Epoch 117/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4323 - acc: 0.8489\n",
            "Epoch 118/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4351 - acc: 0.8472\n",
            "Epoch 119/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4216 - acc: 0.8534\n",
            "Epoch 120/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4294 - acc: 0.8470\n",
            "Epoch 121/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4234 - acc: 0.8485\n",
            "Epoch 122/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4186 - acc: 0.8535\n",
            "Epoch 123/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4188 - acc: 0.8536\n",
            "Epoch 124/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4193 - acc: 0.8494\n",
            "Epoch 125/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4045 - acc: 0.8577\n",
            "Epoch 126/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4041 - acc: 0.8566\n",
            "Epoch 127/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4161 - acc: 0.8537\n",
            "Epoch 128/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3973 - acc: 0.8607\n",
            "Epoch 129/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3990 - acc: 0.8583\n",
            "Epoch 130/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3950 - acc: 0.8609\n",
            "Epoch 131/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3915 - acc: 0.8632\n",
            "Epoch 132/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3964 - acc: 0.8583\n",
            "Epoch 133/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3850 - acc: 0.8643\n",
            "Epoch 134/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3881 - acc: 0.8643\n",
            "Epoch 135/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3828 - acc: 0.8642\n",
            "Epoch 136/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3817 - acc: 0.8657\n",
            "Epoch 137/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3861 - acc: 0.8611\n",
            "Epoch 138/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3703 - acc: 0.8694\n",
            "Epoch 139/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3752 - acc: 0.8706\n",
            "Epoch 140/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3731 - acc: 0.8690\n",
            "Epoch 141/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3676 - acc: 0.8678\n",
            "Epoch 142/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3726 - acc: 0.8693\n",
            "Epoch 143/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3627 - acc: 0.8727\n",
            "Epoch 144/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3520 - acc: 0.8752\n",
            "Epoch 145/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3565 - acc: 0.8745\n",
            "Epoch 146/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3556 - acc: 0.8747\n",
            "Epoch 147/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3483 - acc: 0.8774\n",
            "Epoch 148/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3505 - acc: 0.8766\n",
            "Epoch 149/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3507 - acc: 0.8739\n",
            "Epoch 150/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3512 - acc: 0.8772\n",
            "Epoch 151/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3404 - acc: 0.8784\n",
            "Epoch 152/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3408 - acc: 0.8792\n",
            "Epoch 153/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3335 - acc: 0.8819\n",
            "Epoch 154/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3329 - acc: 0.8833\n",
            "Epoch 155/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3417 - acc: 0.8810\n",
            "Epoch 156/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3395 - acc: 0.8798\n",
            "Epoch 157/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3329 - acc: 0.8826\n",
            "Epoch 158/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3228 - acc: 0.8852\n",
            "Epoch 159/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3308 - acc: 0.8834\n",
            "Epoch 160/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3273 - acc: 0.8833\n",
            "Epoch 161/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3191 - acc: 0.8878\n",
            "Epoch 162/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3114 - acc: 0.8939\n",
            "Epoch 163/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3146 - acc: 0.8887\n",
            "Epoch 164/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3159 - acc: 0.8896\n",
            "Epoch 165/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3098 - acc: 0.8895\n",
            "Epoch 166/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3071 - acc: 0.8917\n",
            "Epoch 167/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3033 - acc: 0.8937\n",
            "Epoch 168/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3059 - acc: 0.8923\n",
            "Epoch 169/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3018 - acc: 0.8945\n",
            "Epoch 170/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3029 - acc: 0.8933\n",
            "Epoch 171/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3015 - acc: 0.8950\n",
            "Epoch 172/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3064 - acc: 0.8905\n",
            "Epoch 173/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2971 - acc: 0.8943\n",
            "Epoch 174/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2979 - acc: 0.8942\n",
            "Epoch 175/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2927 - acc: 0.8966\n",
            "Epoch 176/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2899 - acc: 0.8975\n",
            "Epoch 177/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2848 - acc: 0.8991\n",
            "Epoch 178/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2895 - acc: 0.8970\n",
            "Epoch 179/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2909 - acc: 0.8964\n",
            "Epoch 180/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2759 - acc: 0.9046\n",
            "Epoch 181/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2751 - acc: 0.9034\n",
            "Epoch 182/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2840 - acc: 0.8987\n",
            "Epoch 183/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2795 - acc: 0.9004\n",
            "Epoch 184/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2783 - acc: 0.9020\n",
            "Epoch 185/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2724 - acc: 0.9038\n",
            "Epoch 186/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2763 - acc: 0.9025\n",
            "Epoch 187/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2696 - acc: 0.9052\n",
            "Epoch 188/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2680 - acc: 0.9055\n",
            "Epoch 189/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2639 - acc: 0.9079\n",
            "Epoch 190/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2640 - acc: 0.9074\n",
            "Epoch 191/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2686 - acc: 0.9031\n",
            "Epoch 192/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2631 - acc: 0.9078\n",
            "Epoch 193/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2620 - acc: 0.9075\n",
            "Epoch 194/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2587 - acc: 0.9094\n",
            "Epoch 195/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2544 - acc: 0.9096\n",
            "Epoch 196/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2588 - acc: 0.9076\n",
            "Epoch 197/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2614 - acc: 0.9062\n",
            "Epoch 198/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2540 - acc: 0.9111\n",
            "Epoch 199/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2473 - acc: 0.9123\n",
            "Epoch 200/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2488 - acc: 0.9125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z-VFk79q2ew"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1y9AGqRq2ew",
        "outputId": "6bf2fc3c-6d97-4a16-b590-42651adf5b6d"
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7281 - acc: 0.7813\n",
            "loss = 0.7281002998352051\n",
            "accuracy = 0.7813000082969666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTjKVKhuq2ew"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}